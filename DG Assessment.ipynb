{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd90907a",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3991648",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "from pandas.plotting import scatter_matrix\n",
    "import matplotlib.pyplot as plt # data visualization\n",
    "import seaborn as sns #statistcal data visualization\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.inspection import PartialDependenceDisplay\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6e6805",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77894b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train_test.csv') # loading data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160343e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial Data Overview\n",
    "print(df.head(10))\n",
    "print(df.shape)\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b4e17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking for missing values\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731b0b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate rows in the dataset\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f'Number of duplicate rows: {duplicates}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507d873c",
   "metadata": {},
   "source": [
    "### Convert dates to datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6c8538",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Purchase_Date'] = pd.to_datetime(df['Purchase_Date'])\n",
    "df['Cover_Start_Date'] = pd.to_datetime(df['Cover_Start_Date'])\n",
    "print(df['Purchase_Date'])\n",
    "print(df['Cover_Start_Date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff673e53",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c4a79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Statistical summary of the numerical columns\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bd9763",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation Heatmap\n",
    "corr_matrix = df.corr()\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb1cc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enhanced Correlation Analysis for features vs. Sale_FLag sorted\n",
    "correlation_with_sale = df.corr()['Sale_Flag'].sort_values()\n",
    "print(correlation_with_sale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9540de90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter matrix for a few selected features\n",
    "attributes = ['Claims_Amount', 'Claims_Count', 'Purchase_Price', 'Premium', 'Age']\n",
    "scatter_matrix(df[attributes], figsize=(12, 8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b3635c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualization for the categoroical variables \n",
    "categorical_vars = ['Plan_Flag', 'PriceTest', 'Account', 'Category']\n",
    "for var in categorical_vars:\n",
    "    sns.countplot(x=var, data=df)\n",
    "    plt.title(f'Distribution of {var}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cd244d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Total counts for each category\n",
    "for var in categorical_vars:\n",
    "    print(f\"Counts for {var}: \\n{df[var].value_counts()}\\n\")\n",
    "\n",
    "#Distribution of sales_Flag within each category\n",
    "sale_distribution = df.groupby(var)['Sale_Flag'].value_counts(normalize=True).unstack()*100\n",
    "print(f\"Distribution of Sale_Flag within {var}:\\n{sale_distribution}\\n\")                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2232329f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Boxplots for Catorgorical Variables\n",
    "sns.boxplot(x='Sale_Flag', y='Premium', data=df)\n",
    "plt.title('Premium Distribution by Sale_Flag')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c97242e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing the distribution of numerical features\n",
    "df.hist(bins=50, figsize=(20,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0088e610",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing Key Numerical Feature Distribution\n",
    "df[['Claims_Amount', 'Claims_Count', 'Purchase_Price', 'Age', 'Price_Diff']].hist(bins=30, figsize=(15,10), edgecolor='k')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56979565",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scatter Plots for Key Numerical Relationships\n",
    "sns.scatterplot(x= 'Purchase_Price', y='Premium', hue='Sale_Flag', data=df)\n",
    "plt.title('Purchase Price vs. Premium by Sale_Flag')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f11b0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distribution of the target variable 'Sale_Flag'\n",
    "print(df['Sale_Flag'].value_counts(normalize=True))\n",
    "#visual distribtion of target variables\n",
    "sns.countplot(x='Sale_Flag', data=df)\n",
    "plt.title('Distribution of Sale_Flag')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccff5285",
   "metadata": {},
   "source": [
    "# Price Elasticity Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc512c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Segmenting Data\n",
    "flat_price_df = df[df['PriceTest'] == 0]\n",
    "random_price_df = df[df['PriceTest'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a8ff69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculation of metrics\n",
    "conversion_flat = flat_price_df['Sale_Flag'].mean()\n",
    "conversion_random = random_price_df['Sale_Flag'].mean()\n",
    "avg_premium_flat = flat_price_df['Premium'].mean()\n",
    "avg_premium_random = random_price_df['Premium'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f478f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computing Price Elasticity\n",
    "price_elasticity = ((conversion_random - conversion_flat) / conversion_flat) / ((avg_premium_random - avg_premium_flat) / avg_premium_flat)\n",
    "print(\"Price Elasticity:\", price_elasticity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b0b17d",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc70e0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new Feature \n",
    "# Time-related new feature: Days from purchase to cover start\n",
    "df['Days_till_Cover_Start'] = (df['Cover_Start_Date'] - df['Purchase_Date']).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c706397d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#binning the 'Age' feature\n",
    "kbd = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='quantile')\n",
    "df['Age_binned'] = kbd.fit_transform(df[['Age']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e152316c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping constant and non-predictive features \n",
    "df.drop(['Period_of_Cover', 'PriceTest', 'Purchase_Date', 'Cover_Start_Date'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a52a1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding categorical variables usign dummy variables\n",
    "df_encoded = pd.get_dummies(df, columns=['Account', 'Category'], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d5b14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into training + validation (80%) and test (20%)\n",
    "X = df_encoded.drop('Sale_Flag', axis=1)\n",
    "y = df_encoded['Sale_Flag']\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1485dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further splitting the training + validation set into training (80%) and validation (20%)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=42)  # 0.25 x 0.8 = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfee24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying SMOTE to the training data for adressign the class imbalance\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_sm, y_train_sm = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d8b237",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Class distribution before SMOTE:\")\n",
    "print(y_train.value_counts())\n",
    "print(\"Class distribution after SMOTE:\")\n",
    "print(y_train_sm.value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832e4e77",
   "metadata": {},
   "source": [
    "# Demand Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbe9c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_sm_scaled = scaler.fit_transform(X_train_sm)\n",
    "X_val_scaled = scaler.transform(X_val)  # Scaling the validation set\n",
    "X_test_scaled = scaler.transform(X_test)  # Scaling the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe745fc1",
   "metadata": {},
   "source": [
    "### Initialize models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3773eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models\n",
    "logreg_model = LogisticRegression()\n",
    "knn_model = KNeighborsClassifier()\n",
    "gb_model = GradientBoostingClassifier()\n",
    "rf_model = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3867965e",
   "metadata": {},
   "source": [
    "### Hyperparameter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819a8f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter grids\n",
    "logreg_param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10],\n",
    "    'solver': ['liblinear', 'lbfgs']\n",
    "}\n",
    "\n",
    "knn_param_grid = {\n",
    "    'n_neighbors': [3, 5, 7],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'p': [1, 2]\n",
    "}\n",
    "\n",
    "gb_param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.1, 0.01, 0.001],\n",
    "    'max_depth': [3, 5, 7]\n",
    "}\n",
    "\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 5, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506bae1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparation for gridsearch\n",
    "models = [logreg_model, knn_model, gb_model, rf_model]\n",
    "param_grids = [logreg_param_grid, knn_param_grid, gb_param_grid, rf_param_grid]\n",
    "model_names = ['Logistic Regression', 'KNN', 'Gradient Boosting', 'Random Forest']\n",
    "\n",
    "# Dictionary to hold the best models\n",
    "best_models = {}\n",
    "\n",
    "for model, param_grid, name in zip(models, param_grids, model_names):\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='roc_auc', n_jobs=-1)\n",
    "    grid_search.fit(X_train_sm_scaled, y_train_sm)  \n",
    "    best_models[name] = grid_search.best_estimator_\n",
    "    print(f\"Best parameters for {name}: {grid_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3a5562",
   "metadata": {},
   "source": [
    "## Evaluated each best model on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd70677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists to store evaluation metrics\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "# Iteration over each best model\n",
    "for model_name, best_model in best_models.items():\n",
    "    # Predict on the validation set\n",
    "    y_pred_val = best_model.predict(X_val_scaled)\n",
    "    pos_label_encoded = 1  \n",
    "\n",
    "    # Calculating and storing evaluation metrics\n",
    "    accuracy_scores.append(np.round(accuracy_score(y_val, y_pred_val), 3))\n",
    "    precision_scores.append(np.round(precision_score(y_val, y_pred_val, pos_label=pos_label_encoded), 3))\n",
    "    recall_scores.append(np.round(recall_score(y_val, y_pred_val, pos_label=pos_label_encoded), 3))\n",
    "    f1_scores.append(np.round(f1_score(y_val, y_pred_val, pos_label=pos_label_encoded), 3))\n",
    "\n",
    "    # Print classification report for each model\n",
    "    print(f\"Classification Report - {model_name} Model:\")\n",
    "    print(classification_report(y_val, y_pred_val))\n",
    "\n",
    "    # Confusion matrix heatmap\n",
    "    cm = confusion_matrix(y_val, y_pred_val)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'Confusion Matrix - {model_name} Model')\n",
    "    plt.xlabel('Predicted labels')\n",
    "    plt.ylabel('True labels')\n",
    "    plt.xticks([0, 1], ['Class 0', 'Class 1'])\n",
    "    plt.yticks([0, 1], ['Class 0', 'Class 1'])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafcae40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated list of model names\n",
    "model_names = ['Logistic Regression', 'KNN', 'Gradient Boosting', 'Random Forest']\n",
    "\n",
    "# Updated list of corresponding metrics based on the classification reports\n",
    "accuracy_scores = [0.78, 0.74, 0.75, 0.77]  # Updated accuracy scores\n",
    "precision_scores = [0.45, 0.34, 0.39, 0.44]  # Updated precision scores for class '1'\n",
    "recall_scores = [0.28, 0.15, 0.05, 0.13]  # Updated recall scores for class '1'\n",
    "f1_scores = [0.35, 0.21, 0.09, 0.23]  # Updated F1 scores for class '1'\n",
    "\n",
    "# Setting positions for the bars\n",
    "x = np.arange(len(model_names))\n",
    "width = 0.2\n",
    "\n",
    "# Creating bar plots for each metric\n",
    "plt.figure(figsize=(10, 6))  # Adjusting figure size for better readability\n",
    "plt.bar(x - width*1.5, accuracy_scores, width, label='Accuracy')\n",
    "plt.bar(x - width/2, precision_scores, width, label='Precision')\n",
    "plt.bar(x + width/2, recall_scores, width, label='Recall')\n",
    "plt.bar(x + width*1.5, f1_scores, width, label='F1-Score')\n",
    "\n",
    "# Setting labels and title\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('Scores')\n",
    "plt.title('Model Evaluation Metrics')\n",
    "plt.xticks(x, model_names)\n",
    "plt.legend()\n",
    "\n",
    "# Displaying the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0097020",
   "metadata": {},
   "source": [
    "## Model Comparison using ROC AUC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c19f268",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [logreg_model, knn_model, gb_model, rf_model]\n",
    "model_names = ['Logistic Regression', 'KNN', 'Gradient Boosting', 'Random Forest']\n",
    "\n",
    "# Initializing empty lists to store mean ROC AUC scores for each model\n",
    "mean_roc_auc_scores = []\n",
    "\n",
    "# Performing cross-validation for each model\n",
    "for model in models:\n",
    "    roc_auc_scores = cross_val_score(model, X_train_sm_scaled, y_train_sm, cv=5, scoring='roc_auc', n_jobs=-1)\n",
    "    mean_roc_auc_scores.append(np.mean(roc_auc_scores))\n",
    "\n",
    "# Displaying the mean ROC AUC scores for each model\n",
    "for model_name, roc_auc in zip(model_names, mean_roc_auc_scores):\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Mean ROC AUC Score: {roc_auc}\")\n",
    "    print(\"=============================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7edc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = ['Logistic Regression', 'KNN', 'Gradient Boosting', 'Random Forest']\n",
    "\n",
    "# Mean ROC AUC scores for each model based on results\n",
    "mean_roc_auc_scores = [0.874357756029327, 0.8694375133233214, 0.8729172205575555, 0.912243773068807]\n",
    "\n",
    "# Bar plot for Mean ROC AUC Scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(model_names, mean_roc_auc_scores, color=['blue', 'orange', 'green', 'red'])\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('Mean ROC AUC Score')\n",
    "plt.title('Model Comparison using Mean ROC AUC Scores')\n",
    "plt.ylim([0, 1]) \n",
    "plt.xticks(rotation=45)  # Rotate model names for better readability\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9b62e1",
   "metadata": {},
   "source": [
    "## Best Model Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68479a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model_index = model_names.index('Random Forest')\n",
    "best_rf_model = best_models['Random Forest']  # Directly access by name\n",
    "\n",
    "# Testing the best-performing Random Forest model on the test data\n",
    "y_pred_test_rf = best_rf_model.predict(X_test_scaled)\n",
    "\n",
    "# Classification report for the test set with meaningful class names\n",
    "print(\"Classification Report - Best Random Forest Model (Test Set):\")\n",
    "print(classification_report(y_test, y_pred_test_rf, target_names=['Not Accepted', 'Accepted']))\n",
    "\n",
    "# Confusion matrix heatmap for the test set\n",
    "cm_test_rf = confusion_matrix(y_test, y_pred_test_rf)\n",
    "sns.heatmap(cm_test_rf, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix - Best Random Forest Model (Test Set)')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.xticks([0.5, 1.5], ['Not Accepted', 'Accepted'])\n",
    "plt.yticks([0.5, 1.5], ['Not Accepted', 'Accepted'], rotation=0)  \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747494e9",
   "metadata": {},
   "source": [
    "# Feature Importance and Partial Dependence Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19abd317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'best_rf_model' is trained RandomForest model\n",
    "\n",
    "# Calculation of feature importances and sorting them\n",
    "feature_importances = best_rf_model.feature_importances_\n",
    "features_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': feature_importances})\n",
    "top_features_df = features_df.sort_values(by='Importance', ascending=False).head(5)\n",
    "\n",
    "# Plotting feature importances for top 5 features\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=top_features_df, palette='viridis')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Top 5 Feature Importances from Random Forest Model')\n",
    "plt.show()\n",
    "\n",
    "# Extracting the names of the top 5 features\n",
    "top_features_names = top_features_df['Feature'].tolist()\n",
    "\n",
    "# Generating Partial Dependence Plots for each of the top 5 features\n",
    "for feature_name in top_features_names:\n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "    PartialDependenceDisplay.from_estimator(best_rf_model, X_train, features=[feature_name], ax=ax, feature_names=X_train.columns.tolist())\n",
    "    ax.set_title(f'Partial Dependence Plot for {feature_name}')\n",
    "    plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da246f07",
   "metadata": {},
   "source": [
    "# Impact of 10% Price Increase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e171edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#'X_test_scaled' is a numpy array and the column index of 'Premium' \n",
    "premium_col_index = X_train.columns.get_loc('Premium')  #column index of 'Premium' before scaling\n",
    "\n",
    "# Making a copy of the scaled test dataset\n",
    "X_test_adjusted = X_test_scaled.copy()\n",
    "\n",
    "# Applying a 10% increase to the 'Premium' feature in the adjusted test dataset\n",
    "X_test_adjusted[:, premium_col_index] *= 1.10\n",
    "\n",
    "# Making predictions with the original and adjusted test datasets\n",
    "original_predictions = best_rf_model.predict_proba(X_test_scaled)[:, 1]\n",
    "adjusted_predictions = best_rf_model.predict_proba(X_test_adjusted)[:, 1]\n",
    "\n",
    "# Comparing the average probabilities of acceptance before and after the premium adjustment\n",
    "print(f\"Average probability of acceptance before adjustment: {np.mean(original_predictions)}\")\n",
    "print(f\"Average probability of acceptance after adjustment: {np.mean(adjusted_predictions)}\")\n",
    "\n",
    "# Visualizing the distribution of probabilities before and after the price adjustment\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(original_predictions, bins=50, alpha=0.5, label='Original Probabilities')\n",
    "plt.hist(adjusted_predictions, bins=50, alpha=0.5, label='Adjusted Probabilities', color='red')\n",
    "plt.xlabel('Probability of Acceptance')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Acceptance Probabilities Before and After 10% Premium Increase')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bf180f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
